{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################################################################################\n",
    "############################ Try SGLang and Test Llama3 on MATH  ###################################\n",
    "####################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sglang.utils import (\n",
    "    execute_shell_command,\n",
    "    wait_for_server,\n",
    "    terminate_process,\n",
    "    print_highlight,\n",
    ")\n",
    "\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': '8ae7cd6a70e243779ec3a7b55e90a910', 'object': 'chat.completion', 'created': 1731544482, 'model': 'meta-llama/Meta-Llama-3.1-8B-Instruct', 'choices': [{'index': 0, 'message': {'role': 'assistant', 'content': 'Here are 3 countries and their capitals:\\n\\n1. United States - Washington D.C.\\n2. Japan - Tokyo\\n3. Australia - Canberra'}, 'logprobs': None, 'finish_reason': 'stop', 'matched_stop': 128009}], 'usage': {'prompt_tokens': 43, 'total_tokens': 73, 'completion_tokens': 30, 'prompt_tokens_details': None}}\n"
     ]
    }
   ],
   "source": [
    "url = \"http://localhost:10086/v1/chat/completions\"\n",
    "\n",
    "data = {\n",
    "    \"model\": \"meta-llama/Meta-Llama-3.1-8B-Instruct\",\n",
    "    \"messages\": [\n",
    "        {\"role\": \"user\", \"content\": \"List 3 countries and their capitals.\"}\n",
    "    ]\n",
    "}\n",
    "\n",
    "response = requests.post(url, json=data)\n",
    "print(response.json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatCompletion(id='e100c05c69d14502ad6af3c2f9205d31', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='Here are 3 countries and their capitals:\\n\\n1. Country: Japan\\n   Capital: Tokyo\\n\\n2. Country: Australia\\n   Capital: Canberra\\n\\n3. Country: Brazil\\n   Capital: BrasÃ­lia', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=None), matched_stop=128009)], created=1731542263, model='meta-llama/Meta-Llama-3.1-8B-Instruct', object='chat.completion', service_tier=None, system_fingerprint=None, usage=CompletionUsage(completion_tokens=43, prompt_tokens=48, total_tokens=91, completion_tokens_details=None, prompt_tokens_details=None))\n"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "\n",
    "client = openai.Client(base_url=\"http://127.0.0.1:10086/v1\", api_key=\"None\")\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"meta-llama/Meta-Llama-3.1-8B-Instruct\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant\"},\n",
    "        {\"role\": \"user\", \"content\": \"List 3 countries and their capitals.\"},\n",
    "    ],\n",
    "    temperature=0,\n",
    "    max_tokens=128,\n",
    ")\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load math500-test.jsonl\n",
    "import json\n",
    "\n",
    "data = []\n",
    "with open(\"math500-test.jsonl\", \"r\") as f:\n",
    "    raw_data = f.readlines()\n",
    "\n",
    "for line in raw_data:\n",
    "    data.append(json.loads(line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need to import an evaluator from dart_math\n",
    "# to compare math expressions with ground truth answers\n",
    "\n",
    "from dart_math.eval import EvaluatorMath\n",
    "math_evaluator = EvaluatorMath()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from simple_eval by OpenAI\n",
    "\n",
    "import random\n",
    "import re\n",
    "from typing import Literal\n",
    "\n",
    "import blobfile as bf\n",
    "import pandas\n",
    "\n",
    "import common\n",
    "from common import ANSWER_PATTERN, HTML_JINJA, check_equality\n",
    "from eval_types import Eval, EvalResult, SamplerBase, SingleEvalResult\n",
    "\n",
    "QUERY_TEMPLATE = \"\"\"\n",
    "Solve the following math problem step by step. The last line of your response should be of the form Answer: $ANSWER (without quotes) where $ANSWER is the answer to the problem.\n",
    "\n",
    "{Question}\n",
    "\n",
    "Remember to put your answer on its own line after \"Answer:\", and you do not need to use a \\\\boxed command.\n",
    "\"\"\".strip()\n",
    "\n",
    "\n",
    "class MathEval(Eval):\n",
    "    def __init__(\n",
    "        self,\n",
    "        equality_checker: SamplerBase,\n",
    "        num_examples: int | None = None,\n",
    "        n_repeats: int = 1,\n",
    "        split: Literal[\"math_test\", \"math_500_test\"] = \"my_math_500_test\",  # see readme.md\n",
    "    ):\n",
    "        df = pandas.read_csv(\n",
    "            # bf.BlobFile(f\"https://openaipublic.blob.core.windows.net/simple-evals/{split}.csv\")\n",
    "            bf.BlobFile(f\"{split}.csv\")\n",
    "        )\n",
    "        examples = [row.to_dict() for _, row in df.iterrows()]\n",
    "        if num_examples:\n",
    "            assert n_repeats == 1, \"n_repeats only supported for num_examples = None\"\n",
    "            rng = random.Random(0)\n",
    "            examples = rng.sample(examples, num_examples)\n",
    "        self.examples = examples * n_repeats\n",
    "        self.equality_checker = equality_checker\n",
    "\n",
    "    def __call__(self, sampler: SamplerBase) -> EvalResult:\n",
    "        def fn(row: dict):\n",
    "            prompt_messages = [\n",
    "                sampler._pack_message(content=QUERY_TEMPLATE.format(**row), role=\"user\")\n",
    "            ]\n",
    "            response_text = sampler(prompt_messages)\n",
    "            match = re.search(ANSWER_PATTERN, response_text)\n",
    "            extracted_answer = match.group(1) if match else None\n",
    "            # score = 0 if extracted_answer is None else \\\n",
    "            #     float(check_equality(self.equality_checker, row[\"Answer\"], extracted_answer))\n",
    "            score = 0 if extracted_answer is None else math_evaluator.eq(row[\"Answer\"], extracted_answer)\n",
    "            \n",
    "            # my change: none -> error\n",
    "            html = common.jinja_env.from_string(HTML_JINJA).render(\n",
    "                prompt_messages=prompt_messages,\n",
    "                next_message=dict(content=response_text, role=\"assistant\"),\n",
    "                score=score,\n",
    "                correct_answer=row[\"Answer\"],\n",
    "                extracted_answer=extracted_answer,\n",
    "            )\n",
    "            convo = prompt_messages + [dict(content=response_text, role=\"assistant\")]\n",
    "            return SingleEvalResult(html=html, score=score, convo=convo)\n",
    "\n",
    "        results = common.map_with_progress(fn, self.examples)\n",
    "        return common.aggregate_results(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# also from simple_eval by OpenAI\n",
    "\n",
    "import base64\n",
    "import time\n",
    "from typing import Any\n",
    "\n",
    "import openai\n",
    "from openai import OpenAI\n",
    "\n",
    "from eval_types import MessageList, SamplerBase\n",
    "\n",
    "OPENAI_SYSTEM_MESSAGE_API = \"You are a helpful assistant.\"\n",
    "OPENAI_SYSTEM_MESSAGE_CHATGPT = (\n",
    "    \"You are ChatGPT, a large language model trained by OpenAI, based on the GPT-4 architecture.\"\n",
    "    + \"\\nKnowledge cutoff: 2023-12\\nCurrent date: 2024-04-01\"\n",
    ")\n",
    "\n",
    "\n",
    "class ChatCompletionSampler(SamplerBase):\n",
    "    \"\"\"\n",
    "    Sample from OpenAI's chat completion API\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        model: str = \"gpt-3.5-turbo\",\n",
    "        system_message: str | None = None,\n",
    "        temperature: float = 0.5,\n",
    "        max_tokens: int = 1024,\n",
    "        client = None, \n",
    "        return_full_response: bool = False,\n",
    "    ):\n",
    "        self.api_key_name = \"OPENAI_API_KEY\"\n",
    "        self.client = client or OpenAI()\n",
    "        # using api_key=os.environ.get(\"OPENAI_API_KEY\")  # please set your API_KEY\n",
    "        self.model = model\n",
    "        self.system_message = system_message\n",
    "        self.temperature = temperature\n",
    "        self.max_tokens = max_tokens\n",
    "        self.image_format = \"url\"\n",
    "        self.return_full_response = return_full_response\n",
    "\n",
    "    def _handle_image(\n",
    "        self, image: str, encoding: str = \"base64\", format: str = \"png\", fovea: int = 768\n",
    "    ):\n",
    "        new_image = {\n",
    "            \"type\": \"image_url\",\n",
    "            \"image_url\": {\n",
    "                \"url\": f\"data:image/{format};{encoding},{image}\",\n",
    "            },\n",
    "        }\n",
    "        return new_image\n",
    "\n",
    "    def _handle_text(self, text: str):\n",
    "        return {\"type\": \"text\", \"text\": text}\n",
    "\n",
    "    def _pack_message(self, role: str, content: Any):\n",
    "        return {\"role\": str(role), \"content\": content}\n",
    "\n",
    "    def __call__(self, message_list: MessageList) -> str:\n",
    "        if self.system_message:\n",
    "            message_list = [self._pack_message(\"system\", self.system_message)] + message_list\n",
    "        trial = 0\n",
    "        # print(message_list)\n",
    "        while True:\n",
    "            try:\n",
    "                response = self.client.chat.completions.create(\n",
    "                    model=self.model,\n",
    "                    messages=message_list,\n",
    "                    temperature=self.temperature,\n",
    "                    max_tokens=self.max_tokens,\n",
    "                )\n",
    "                if self.return_full_response:\n",
    "                    print(\"message\", message_list, \"response\", response.choices[0].message.content)\n",
    "                    return response.choices[0].message.content\n",
    "                else:\n",
    "                    return response.choices[0].message.content\n",
    "\n",
    "            # NOTE: BadRequestError is triggered once for MMMU, please uncomment if you are reruning MMMU\n",
    "            except openai.BadRequestError as e:\n",
    "                print(\"Bad Request Error\", e)\n",
    "                return \"\"\n",
    "            except Exception as e:\n",
    "                exception_backoff = 2**trial  # expontial back off\n",
    "                print(\n",
    "                    f\"Rate limit exception so wait and retry {trial} after {exception_backoff} sec\",\n",
    "                    e,\n",
    "                )\n",
    "                time.sleep(exception_backoff)\n",
    "                trial += 1\n",
    "            # unknown error shall throw exception"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use our sglang client as OpenAI client\n",
    "client = openai.Client(base_url=\"http://127.0.0.1:10086/v1\", api_key=\"None\")\n",
    "\n",
    "sampler = ChatCompletionSampler(model=\"meta-llama/Meta-Llama-3.1-8B-Instruct\",\n",
    "                                temperature=0.0,\n",
    "                                max_tokens=2048,\n",
    "                                client=client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "debug = False\n",
    "# equality_checker = sampler\n",
    "matheval = MathEval(\n",
    "     equality_checker=sampler, num_examples=10 if debug else 500\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "samplers = {\"llama3\": sampler}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'matheval': <__main__.MathEval object at 0x7f46d1e51c00>}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|ââââââââââ| 500/500 [02:59<00:00,  2.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing report to /tmp/matheval_llama3.html\n",
      "{'score:std': 0.4976906669810074, 'score': 0.452}\n",
      "Writing results to /tmp/matheval_llama3.json\n",
      "\n",
      "All results: \n",
      "| sampler_name   |   ('metric', 'matheval') |\n",
      "|:---------------|-------------------------:|\n",
      "| llama3         |                    0.452 |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# also from simple_eval by OpenAI\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "evals = {\n",
    "    \"matheval\": matheval\n",
    "}\n",
    "print(evals)\n",
    "debug_suffix = \"_DEBUG\" if debug else \"\"\n",
    "print(debug_suffix)\n",
    "mergekey2resultpath = {}\n",
    "for sampler_name, sampler in samplers.items():\n",
    "    for eval_name, eval_obj in evals.items():\n",
    "        result = eval_obj(sampler)\n",
    "        # ^^^ how to use a sampler\n",
    "        file_stem = f\"{eval_name}_{sampler_name}\"\n",
    "        report_filename = f\"/tmp/{file_stem}{debug_suffix}.html\"\n",
    "        print(f\"Writing report to {report_filename}\")\n",
    "        with open(report_filename, \"w\") as fh:\n",
    "            fh.write(common.make_report(result))\n",
    "        metrics = result.metrics | {\"score\": result.score}\n",
    "        print(metrics)\n",
    "        result_filename = f\"/tmp/{file_stem}{debug_suffix}.json\"\n",
    "        with open(result_filename, \"w\") as f:\n",
    "            f.write(json.dumps(metrics, indent=2))\n",
    "        print(f\"Writing results to {result_filename}\")\n",
    "        mergekey2resultpath[f\"{file_stem}\"] = result_filename\n",
    "merge_metrics = []\n",
    "for eval_sampler_name, result_filename in mergekey2resultpath.items():\n",
    "    try:\n",
    "        result = json.load(open(result_filename, \"r+\"))\n",
    "    except Exception as e:\n",
    "        print(e, result_filename)\n",
    "        continue\n",
    "    result = result.get(\"f1_score\", result.get(\"score\", None))\n",
    "    eval_name = eval_sampler_name[: eval_sampler_name.find(\"_\")]\n",
    "    sampler_name = eval_sampler_name[eval_sampler_name.find(\"_\") + 1 :]\n",
    "    merge_metrics.append(\n",
    "        {\"eval_name\": eval_name, \"sampler_name\": sampler_name, \"metric\": result}\n",
    "    )\n",
    "merge_metrics_df = pd.DataFrame(merge_metrics).pivot(\n",
    "    index=[\"sampler_name\"], columns=\"eval_name\"\n",
    ")\n",
    "print(\"\\nAll results: \")\n",
    "print(merge_metrics_df.to_markdown())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################################################################################\n",
    "###################################### SGLang for Tree Search ######################################\n",
    "####################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import re\n",
    "from typing import Literal\n",
    "\n",
    "import blobfile as bf\n",
    "import pandas\n",
    "\n",
    "import common\n",
    "from common import ANSWER_PATTERN, HTML_JINJA, check_equality\n",
    "from eval_types import Eval, EvalResult, SamplerBase, SingleEvalResult\n",
    "\n",
    "split = \"my_math_500_test\"\n",
    "df = pandas.read_csv(\n",
    "    # bf.BlobFile(f\"https://openaipublic.blob.core.windows.net/simple-evals/{split}.csv\")\n",
    "    bf.BlobFile(f\"{split}.csv\")\n",
    ")\n",
    "examples = [row.to_dict() for _, row in df.iterrows()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "## try to use sglang to sequentially generate next steps.\n",
    "\n",
    "import sglang as sgl\n",
    "import argparse\n",
    "from sglang.test.test_utils import (\n",
    "    add_common_sglang_args_and_parse,\n",
    "    select_sglang_backend,\n",
    ")\n",
    "import time\n",
    "\n",
    "\n",
    "max_steps = 30\n",
    "\n",
    "\n",
    "@sgl.function\n",
    "def search_try(s, question):\n",
    "    s += sgl.user(\n",
    "        f\"\"\"Solve the following math problem step by step. Steps should be separated with two new lines. The last line of your response should be of the form Answer: $ANSWER (without quotes) where $ANSWER is the answer to the problem.\n",
    "        \n",
    "{question}\n",
    "\n",
    "Remember to separate steps with two new lines, and finally put your answer on its own line after \"Answer:\", and you do not need to use a \\\\boxed command.\n",
    "\"\"\"\n",
    "    )\n",
    "    \n",
    "    s += sgl.assistant_begin()\n",
    "    s += sgl.gen(max_tokens=256, stop=[\"\\n\\n\"])\n",
    "    \n",
    "    # print(f\"''{s.text()}''\")\n",
    "    \n",
    "    for _ in range(max_steps):\n",
    "        # s += new_line\n",
    "        s += \"\\n\\n\"\n",
    "        s += sgl.gen(max_tokens=256, stop=[\"\\n\\n\"])\n",
    "        # print(f\"''{s.text()}''\")  \n",
    "        if \"Answer:\" in s.text().split(\"\\n\")[-1]:\n",
    "            break\n",
    "    \n",
    "    # s += sgl.assistant(sgl.gen(\"step\", max_tokens=256, temperature=0.3, stop=[\"\\n\\n\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|ââââââââââ| 100/100 [01:15<00:00,  1.32it/s]\n"
     ]
    }
   ],
   "source": [
    "args = argparse.Namespace(\n",
    "    # data_path=\"sglang/benchmark/tree_of_thought_v0/test.jsonl\",\n",
    "    # num_questions=2,\n",
    "    port=10086,\n",
    "    parallel=16,\n",
    "    backend='srt',\n",
    "    host=\"http://127.0.0.1\",\n",
    "    result_file=\"results.txt\"\n",
    ")\n",
    "\n",
    "# q = \"\"\"If $x^3$ is a positive factor of $10!,$ how many possible integer values of $x$ are there?  (Reminder: For a positive integer $n$, the expression $n!$ stands for the product of the integers from 1 up to (and including) $n$.)\"\"\"\n",
    "\n",
    "arguments = [{\"question\": d['Question']} for d in examples[:100]]\n",
    "\n",
    "# Select backend\n",
    "backend = select_sglang_backend(args)\n",
    "\n",
    "# Run requests\n",
    "tic = time.time()\n",
    "states = search_try.run_batch(\n",
    "    arguments,\n",
    "    temperature=0,\n",
    "    backend=backend,\n",
    "    num_threads=args.parallel,\n",
    "    progress_bar=True,\n",
    ")\n",
    "latency = time.time() - tic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "$\\left(3, \\frac{\\pi}{2}\\right)$ || \\left( 3, \\frac{\\pi}{2} \\right) || True\n",
      "None || p - q || 0\n",
      "$\\frac{14}{3}$ || \\frac{14}{3} || True\n",
      "9 || 9 || True\n",
      "Evelyn || \\text{Evelyn} || True\n",
      "126 || 42 || False\n",
      "9 || 27 || False\n",
      "$20$ || 90^\\circ || False\n",
      "3â13 || 3\\sqrt{13} || False\n",
      "None || 4 || 0\n",
      "None || 2220 || 0\n",
      "$\\frac{1}{8}$ || \\frac{3}{56} || False\n",
      "284 || 284 || True\n",
      "$5$ || 5 || True\n",
      "$10$ || \\sqrt{51} || False\n",
      "$2 - \\frac{11 \\sqrt{2}}{2} + ( -3 + 2 \\sqrt{2} )i$ || 6 - 5i || False\n",
      "-50 || -50 || True\n",
      "$\\pi$ || \\pi || True\n",
      "$292$ || 28 || False\n",
      "None || 3 || 0\n",
      "6 + 9i || 6+9i || True\n",
      "None || 13535 || 0\n",
      "None || 5 || 0\n",
      "None || x=5 || 0\n",
      "0.6 || 10 || False\n",
      "$\\boxed{\\text{There are no solutions.}}$ || 1,-2 || False\n",
      "20 || 144 || False\n",
      "$78 || 78 || True\n",
      "-2 + 7i || -2 + 7i || True\n",
      "None || 225 || 0\n",
      "$52_8$ || 52_8 || True\n",
      "11$\\sqrt{2}$ || 11\\sqrt2 || True\n",
      "None || 720 || 0\n",
      "None || \\frac{243}{625} || 0\n",
      "-125 || -125 || True\n",
      "3 || 3 || True\n",
      "$2, 5$ || 3, 5, 7 || False\n",
      "72 || 72 || True\n",
      "2000 || 2000 || True\n",
      "23 || 23 || True\n",
      "12 || 12 || True\n",
      "17 || 17 || True\n",
      "4 || 4 || True\n",
      "140 || 70 \\sqrt{2} || False\n",
      "1.25 || 1.25 || True\n",
      "2 || 2 || True\n",
      "12 || 6 || False\n",
      "5 || 5 || True\n",
      "3/2 || \\frac{3}{2} || True\n",
      "$83$ || 83 || True\n",
      "None || 203 || 0\n",
      "$x^5 - x^4 + x^3 - x^2 - x - 1$ || x^5 - x^4 + x^3 - x^2 + x - 1 || False\n",
      "12 || 12 || True\n",
      "$-\\frac{\\pi}{6}$ || -\\frac{\\pi}{6} || True\n",
      "0.15 || 0.15 || True\n",
      "4 || 3 || False\n",
      "11 || 11 || True\n",
      "256 || 16 || False\n",
      "$9702 || 9901 || False\n",
      "None || 5 || 0\n",
      "(3,13,0) || (6,31,-1) || False\n",
      "$-256$ || -256 || True\n",
      "$\\boxed{\\sqrt{13}}$ || 4 || False\n",
      "10 || 10 || True\n",
      "None || \\frac{35}{64} || 0\n",
      "None || 1 || 0\n",
      "x^3 - 3x^2 + 6x - 6 || x^3+3x-6 || False\n",
      "10 || 10 || True\n",
      "46 || 46 || True\n",
      "None || -1 || 0\n",
      "60_9 || 40_9 || False\n",
      "$2152_{8}$ || 2516_8 || False\n",
      "3 || 3 || True\n",
      "$\\frac{3\\sqrt{3}}{4}$ || \\frac{3\\sqrt{3}}{4} || True\n",
      "csc x || \\cot x || False\n",
      "5/9 || \\frac{11}{36} || False\n",
      "$0$ || 0 || True\n",
      "$4$ || 4 || True\n",
      "$\\left(-\\frac{6}{7}, \\frac{3}{7}\\right)$ || (-2,1) || False\n",
      "2 || 2 || True\n",
      "None || 501 || 0\n",
      "3 || 3 || True\n",
      "$\\frac{3}{2}$ || \\frac{3}{2} || True\n",
      "None || 2 || 0\n",
      "$-1$ || -1 || True\n",
      "$\\boxed{\\sqrt{5}}$ || \\sqrt{5} || False\n",
      "240 || 240 || True\n",
      "1 || 1 || True\n",
      "None || 2 || 0\n",
      "3 || 21 || False\n",
      "$15$ || \\frac{3}{2} || False\n",
      "3 || 1 || False\n",
      "0.0286 || \\frac{448}{15625} || False\n",
      "$33$ || 33 || True\n",
      "30 || 80 || False\n",
      "-4 || -4 || True\n",
      "None || 1 \\pm \\sqrt{19} || 0\n",
      "south || \\text{east} || False\n",
      "1 || 2k+2 || False\n",
      "$\\boxed{\\begin{pmatrix} -1 \\\\ 1 \\\\ 2 \\end{pmatrix}}$ || \\begin{pmatrix} -1/3 \\\\ 2/3 \\\\ 5/3 \\end{pmatrix} || False\n",
      "0.46\n"
     ]
    }
   ],
   "source": [
    "# for i, state in enumerate(states):\n",
    "#     print(f\"Question: {arguments[i]['question']}\")\n",
    "#     print(f\"Answer: {state.text()}\")\n",
    "\n",
    "scores = 0\n",
    "for i, state in enumerate(states):\n",
    "    response_text = state.text()\n",
    "    match = re.search(ANSWER_PATTERN, response_text.split(\"\\n\")[-1])\n",
    "    extracted_answer = match.group(1) if match else None\n",
    "    # score = 0 if extracted_answer is None else \\\n",
    "    #     float(check_equality(self.equality_checker, row[\"Answer\"], extracted_answer))\n",
    "    answer = examples[i][\"Answer\"]\n",
    "    score = 0 if extracted_answer is None else math_evaluator.eq(answer, extracted_answer)\n",
    "    \n",
    "    print(f\"{extracted_answer} || {answer} || {score}\")\n",
    "    scores += score\n",
    "    \n",
    "print(scores/len(states))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Try beam search\n",
    "\n",
    "import sglang as sgl\n",
    "max_steps = 30\n",
    "\n",
    "BEAM_SIZE = 4\n",
    "BEAM_WIDTH = 2\n",
    "assert BEAM_SIZE % BEAM_WIDTH == 0\n",
    "\n",
    "@sgl.function\n",
    "def beam_search(s, question):\n",
    "    s += sgl.user(\n",
    "        f\"\"\"Solve the following math problem step by step. Steps should be separated with two new lines. The last line of your response should be of the form Answer: $ANSWER (without quotes) where $ANSWER is the answer to the problem.\n",
    "        \n",
    "{question}\n",
    "\n",
    "Remember to separate steps with two new lines, and finally put your answer on its own line after \"Answer:\", and you do not need to use a \\\\boxed command.\"\"\"\n",
    "    )\n",
    "\n",
    "    s += sgl.assistant_begin()\n",
    "    forks = s.fork(BEAM_SIZE)\n",
    "    forks += sgl.gen(max_tokens=256, stop=[\"\\n\\n\"], temperature=0.5)\n",
    "    step = s.text().split(\"\\n\\n\")[-1]\n",
    "\n",
    "    # print(f\"''{s.text()}''\")\n",
    "    cur_states = list(forks)\n",
    "    \n",
    "    answer_states = []\n",
    "\n",
    "    for _ in range(max_steps):\n",
    "        \n",
    "        # s += new_line\n",
    "        # randomly select BEAM_WIDTH states\n",
    "        # print(\"--A--\")\n",
    "        \n",
    "        cur_beam_width = min(BEAM_WIDTH, len(cur_states))\n",
    "        \n",
    "        cur_states = random.sample(cur_states, cur_beam_width)\n",
    "\n",
    "        # expand to BEAM_SIZE states\n",
    "        new_states = []\n",
    "\n",
    "        for state in cur_states:\n",
    "\n",
    "            # print(\"--B--\")\n",
    "\n",
    "            if \"Answer:\" in state.text().split(\"\\n\")[-1]:\n",
    "                answer_states.append(state)\n",
    "                continue\n",
    "            \n",
    "            # print(\"--C--\")\n",
    "            forked_states = state.fork((BEAM_SIZE - 1) // cur_beam_width + 1)\n",
    "            forked_states += \"\\n\\n\" + sgl.gen(max_tokens=256, stop=[\"\\n\\n\"], temperature=0.5)\n",
    "            new_states.extend(forked_states)\n",
    "        \n",
    "        # print(\"--D--\")\n",
    "        # print(len(new_states))\n",
    "        cur_states = new_states\n",
    "        \n",
    "        if len(answer_states) > 0:\n",
    "            break\n",
    "            \n",
    "        \n",
    "    return answer_states\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|ââââââââââ| 10/10 [00:35<00:00,  3.54s/it]\n"
     ]
    }
   ],
   "source": [
    "bs_states = beam_search.run_batch(\n",
    "    arguments[:10],\n",
    "    temperature=0,\n",
    "    backend=backend,\n",
    "    num_threads=args.parallel,\n",
    "    progress_bar=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, \\frac{\\pi}{2}) || \\left( 3, \\frac{\\pi}{2} \\right) || True\n",
      "p q (1 - 3 p^2 + 3 p r) - s p^3 || p - q || False\n",
      "$\\frac{14}{3}$ || \\frac{14}{3} || True\n",
      "9 || 9 || True\n",
      "Angela || \\text{Evelyn} || False\n",
      "42 || 42 || True\n",
      "27 || 27 || True\n",
      "98.04 || 90^\\circ || False\n",
      "$3\\sqrt{13}$ || 3\\sqrt{13} || True\n",
      "32 || 4 || False\n",
      "0.6\n"
     ]
    }
   ],
   "source": [
    "scores = 0\n",
    "\n",
    "for i, states in enumerate(bs_states):\n",
    "    if len(states.ret_value) == 0:\n",
    "        continue\n",
    "\n",
    "    response_text = states.ret_value[0].text()\n",
    "    match = re.search(ANSWER_PATTERN, response_text.split(\"\\n\")[-1])\n",
    "    extracted_answer = match.group(1) if match else None\n",
    "    # score = 0 if extracted_answer is None else \\\n",
    "    #     float(check_equality(self.equality_checker, row[\"Answer\"], extracted_answer))\n",
    "    answer = examples[i][\"Answer\"]\n",
    "    score = 0 if extracted_answer is None else math_evaluator.eq(answer, extracted_answer)\n",
    "    \n",
    "    print(f\"{extracted_answer} || {answer} || {score}\")\n",
    "    scores += score\n",
    "\n",
    "print(scores/len(bs_states))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ProgramState(<|start_header_id|>user<|end_header_id|>\n",
       "\n",
       "Solve the following math problem step by step. Steps should be separated with two new lines. The last line of your response should be of the form Answer: $ANSWER (without quotes) where $ANSWER is the answer to the problem.\n",
       "        \n",
       "Convert the point $(0,3)$ in rectangular coordinates to polar coordinates.  Enter your answer in the form $(r,\\theta),$ where $r > 0$ and $0 \\le \\theta < 2 \\pi.$\n",
       "\n",
       "Remember to separate steps with two new lines, and finally put your answer on its own line after \"Answer:\", and you do not need to use a \\boxed command.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
       "\n",
       ")"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bs_states[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|ââââââââââ| 100/100 [01:48<00:00,  1.08s/it]\n"
     ]
    }
   ],
   "source": [
    "arguments = [{\"question\": d['Question']} for d in examples[:100]]\n",
    "\n",
    "# Select backend\n",
    "backend = select_sglang_backend(args)\n",
    "\n",
    "# Run requests\n",
    "tic = time.time()\n",
    "states = search_try.run_batch(\n",
    "    arguments,\n",
    "    temperature=0,\n",
    "    backend=backend,\n",
    "    num_threads=args.parallel,\n",
    "    progress_bar=True,\n",
    ")\n",
    "latency = time.time() - tic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, \\frac{\\pi}{2}) || \\left( 3, \\frac{\\pi}{2} \\right) || True\n",
      "None || p - q || 0\n",
      "$\\frac{14}{3}$ || \\frac{14}{3} || True\n",
      "9 || 9 || True\n",
      "Angela || \\text{Evelyn} || False\n",
      "126 || 42 || False\n",
      "None || 27 || 0\n",
      "$20$ || 90^\\circ || False\n",
      "3â13 || 3\\sqrt{13} || False\n",
      "None || 4 || 0\n",
      "None || 2220 || 0\n",
      "125/168 || \\frac{3}{56} || False\n",
      "284 || 284 || True\n",
      "$5$ || 5 || True\n",
      "$10$ || \\sqrt{51} || False\n",
      "None || 6 - 5i || 0\n",
      "-50 || -50 || True\n",
      "$\\pi$ || \\pi || True\n",
      "112 || 28 || False\n",
      "None || 3 || 0\n",
      "6 + 9i || 6+9i || True\n",
      "None || 13535 || 0\n",
      "None || 5 || 0\n",
      "5 || x=5 || True\n",
      "10 || 10 || True\n",
      "$\\boxed{}$ || 1,-2 || False\n",
      "12 || 144 || False\n",
      "$78 || 78 || True\n",
      "-2 + 7i || -2 + 7i || True\n",
      "112 || 225 || False\n",
      "$2_8$ || 52_8 || False\n",
      "11$\\sqrt{2}$ || 11\\sqrt2 || True\n",
      "None || 720 || 0\n",
      "None || \\frac{243}{625} || 0\n",
      "$-\\frac{1}{32}$ || -125 || False\n",
      "3 || 3 || True\n",
      "$2, 5$ || 3, 5, 7 || False\n",
      "360 || 72 || False\n",
      "2000 || 2000 || True\n",
      "23 || 23 || True\n",
      "12 || 12 || True\n",
      "17 || 17 || True\n",
      "4 || 4 || True\n",
      "None || 70 \\sqrt{2} || 0\n",
      "1.25 || 1.25 || True\n",
      "2 || 2 || True\n",
      "12 || 6 || False\n",
      "5 || 5 || True\n",
      "3/2 || \\frac{3}{2} || True\n",
      "$65$ || 83 || False\n",
      "0.46\n"
     ]
    }
   ],
   "source": [
    "# for i, state in enumerate(states):\n",
    "#     print(f\"Question: {arguments[i]['question']}\")\n",
    "#     print(f\"Answer: {state.text()}\")\n",
    "\n",
    "scores = 0\n",
    "for i, state in enumerate(states):\n",
    "    response_text = state.text()\n",
    "    match = re.search(ANSWER_PATTERN, response_text.split(\"\\n\")[-1])\n",
    "    extracted_answer = match.group(1) if match else None\n",
    "    # score = 0 if extracted_answer is None else \\\n",
    "    #     float(check_equality(self.equality_checker, row[\"Answer\"], extracted_answer))\n",
    "    answer = examples[i][\"Answer\"]\n",
    "    score = 0 if extracted_answer is None else math_evaluator.eq(answer, extracted_answer)\n",
    "    \n",
    "    print(f\"{extracted_answer} || {answer} || {score}\")\n",
    "    scores += score\n",
    "    \n",
    "print(scores/len(states))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sgl-py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
